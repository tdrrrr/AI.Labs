
Computer architects use parallelism and various strategies for memory organization to design computing systems with very high performance.
Hack into this quiz and let some technology tally your score and reveal the contents to you.
Computers also have another level of memory called a cache, a small, extremely fast (compared with the main memory, or random access memory [RAM]) unit that can be used to store information that is urgently or frequently needed.
Current research includes cache design and algorithms that can predict what data is likely to be needed next and preload it into the cache for improved performance.
Finally, the instruction STORE 20 would store that sum into location 20.
Most importantly, computer languages include conditional instructions—essentially, rules that say, “If memory location n satisfies condition a, do instruction number x next, otherwise do instruction y.” This allows the course of a program to be determined by the results of previous operations—a critically important ability.
For example, a loop would be needed to compute the sum of the first n integers, where n is a value stored in a separate memory location.
Computer architectures that can execute sequences of instructions, conditional instructions, and loops are called “Turing complete,” which means that they can carry out the execution of any algorithm that can be defined.
Logic design is the area of computer science that deals with the design of electronic circuits using the fundamental principles and properties of logic (see Boolean algebra) to carry out the operations of the control unit, the ALU, the I/O controllers, and other hardware.
For example, the addition circuit of the ALU has inputs corresponding to all the bits of the two numbers to be added and outputs corresponding to the bits of the sum.
Due to the need for efficiency, logic design must also optimize the circuitry to function with maximum speed and has a minimum number of gates and circuits.
An important area related to architecture is the design of microprocessors, which are complete CPUs—control unit, ALU, and memory—on a single integrated circuit chip.
Additional memory and I/O control circuitry are linked to this chip to form a complete computer.
It also involves chip planning, which includes building a “floor plan” to indicate where on the chip each component should be placed and connected to other components.
Computer scientists are also involved in creating the computer-aided design (CAD) tools that support engineers in the various stages of chip design and in developing the necessary theoretical results, such as how to efficiently design a floor plan with near-minimal area that satisfies the given constraints.
Fault tolerance is the ability of a computer to continue operation when one or more of its components fails.
Such applications as aircraft control and manufacturing process control run on systems with backup processors ready to take over if the main processor fails, and the backup systems often run in parallel so the transition is smooth.
If the systems are critical in that their failure would be potentially disastrous (as in aircraft control), incompatible outcomes collected from replicated processes running in parallel on separate machines are resolved by a voting mechanism.
Computer scientists are involved in the analysis of such replicated systems, providing theoretical approaches to estimating the reliability achieved by a given configuration and processor parameters, such as average time between failures and average time required to repair the processor.
For example, an advantage of a distributed database is that data replicated on different network hosts can provide a natural backup mechanism when one host fails.
When discretized in this way, many problems can be recast as an equation involving a matrix (a rectangular array of numbers) solvable using linear algebra.
Several factors must be considered when applying numerical methods: (1) the conditions under which the method yields a solution, (2) the accuracy of the solution, (3) whether the solution process is stable (i.e., does not exhibit error growth), and (4) the computational complexity (in the sense described above) of obtaining a solution of the desired accuracy.
The requirements of big-data scientific problems, including the solution of ever larger systems of equations, engage the use of large and powerful arrays of processors (called multiprocessors or supercomputers) that allow many calculations to proceed in parallel by assigning them to separate processing elements.
CAD systems allow the computer to be used for designing objects ranging from automobile parts to bridges to computer chips by providing an interactive drawing tool and an engineering interface to simulation and analysis tools.
Fine arts applications allow artists to use the computer screen as a medium to create images, cinematographic special effects, animated cartoons, and television commercials.
Scientific visualization uses massive amounts of data to define simulations of scientific phenomena, such as ocean modeling, to produce pictures that provide more insight into the phenomena than would tables of numbers.
The term virtual reality has been coined to refer to any interaction with a computer-simulated virtual world.
In order for realistic on-screen images to be presented, each object must be rendered as a set of planar units.
Both extremely efficient algorithms and state-of-the-art hardware are needed to accomplish real-time animation.
User interface design impacts the life cycle of software, so it should occur early in the design process.
In the 1960s, user interfaces consisted of computer consoles that allowed an operator directly to type commands that could be executed immediately or at some future time.
With the advent of more user-friendly personal computers in the 1980s, user interfaces became more sophisticated, so that the user could “point and click” to send a command to the operating system.
Thus, the field of HCI emerged to model, develop, and measure the effectiveness of various types of interfaces between a computer application and the person accessing its services.
This technology also supports windowing environments on a computer screen, which allow users to work with different applications simultaneously, one in each window.